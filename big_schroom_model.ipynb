{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkuczynski11/micropython-tflite/blob/model_creation_update/big_schroom_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFMlPQlLucMd"
      },
      "source": [
        "### Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l8AjCQHIjBT8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voUgY4blulsN"
      },
      "source": [
        "### Setup training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lX_31Jsxul_m"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 96\n",
        "img_width = 96\n",
        "# Use ready cnn architecture to detect image features\n",
        "# cnn_feature_vector = \"https://tfhub.dev/google/imagenet/mobilenet_v2_035_224/feature_vector/5\"\n",
        "# cnn_feature_vector = \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\"\n",
        "# cnn_feature_vector = \"https://tfhub.dev/google/imagenet/mobilenet_v1_025_224/feature_vector/5\"\n",
        "cnn_feature_vector = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_96/feature_vector/5\"\n",
        "quantization = 'float' # Possible ['none', 'dynamic', 'float', 'int']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgJO7H5qupL4",
        "outputId": "9573a609-14b3-4799-c8bc-054eaf4051cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Found 28045 files belonging to 17 classes.\n",
            "Using 22436 files for training.\n"
          ]
        }
      ],
      "source": [
        "# Usage of google drive where we are storing our data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# data_dir = '/content/gdrive/MyDrive/Mushrooms' # directory of your data\n",
        "# !rm -rf /content/gdrive/MyDrive/Mushrooms/.ipynb_checkpoints/\n",
        "\n",
        "# data_dir = '/content/gdrive/MyDrive/flowers' # directory of your data\n",
        "# !rm -rf /content/gdrive/MyDrive/flowers/.ipynb_checkpoints/\n",
        "\n",
        "data_dir = '/content/gdrive/MyDrive/vehicles' # directory of your data\n",
        "!rm -rf /content/gdrive/MyDrive/vehicles/.ipynb_checkpoints/\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIh6O0CAutsp",
        "outputId": "279aa5c8-81e8-4916-fa1a-737745297010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28045 files belonging to 17 classes.\n",
            "Using 5609 files for validation.\n"
          ]
        }
      ],
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlw3MLtruuwt",
        "outputId": "2cd51e97-225f-46ed-ed06-894816dacffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ambulance', 'Barge', 'Bicycle', 'Boat', 'Bus', 'Car', 'Cart', 'Caterpillar', 'Helicopter', 'Limousine', 'Motorcycle', 'Segway', 'Snowmobile', 'Tank', 'Taxi', 'Truck', 'Van']\n"
          ]
        }
      ],
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWZdeeVTuvG8"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIZ75KRNuxKh",
        "outputId": "ead01475-f4b1-42c5-d4d0-9e4d812a934d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mobilenet_v2_100_96\n",
            "/tmp/models/mobilenet_v2_100_96\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Name model with used architecture\n",
        "model_name = cnn_feature_vector.split('/')[5]\n",
        "print(model_name)\n",
        "model_dir = pathlib.Path(f'/tmp/models/{model_name}')\n",
        "model_dir.mkdir(exist_ok=True, parents=True)\n",
        "print(model_dir)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    hub.KerasLayer(cnn_feature_vector, trainable=False),\n",
        "    tf.keras.layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpkHno5Bu1b3"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehcDfPkDuzV2",
        "outputId": "f2f46cca-47e3-492f-83ed-f716cc7f0f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "702/702 [==============================] - 9028s 13s/step - loss: 0.6020 - accuracy: 0.8144 - val_loss: 0.4790 - val_accuracy: 0.8536\n",
            "Epoch 2/20\n",
            "702/702 [==============================] - 379s 535ms/step - loss: 0.3950 - accuracy: 0.8695 - val_loss: 0.4635 - val_accuracy: 0.8577\n",
            "Epoch 3/20\n",
            "702/702 [==============================] - 382s 540ms/step - loss: 0.3358 - accuracy: 0.8865 - val_loss: 0.4722 - val_accuracy: 0.8584\n",
            "Epoch 4/20\n",
            "702/702 [==============================] - 388s 550ms/step - loss: 0.2985 - accuracy: 0.8988 - val_loss: 0.4874 - val_accuracy: 0.8533\n",
            "Epoch 5/20\n",
            "702/702 [==============================] - 397s 561ms/step - loss: 0.2772 - accuracy: 0.9052 - val_loss: 0.5008 - val_accuracy: 0.8495\n",
            "Epoch 6/20\n",
            "702/702 [==============================] - 985s 1s/step - loss: 0.2532 - accuracy: 0.9131 - val_loss: 0.5107 - val_accuracy: 0.8481\n",
            "Epoch 7/20\n",
            "702/702 [==============================] - 948s 1s/step - loss: 0.2389 - accuracy: 0.9186 - val_loss: 0.5374 - val_accuracy: 0.8415\n",
            "Epoch 8/20\n",
            "702/702 [==============================] - 374s 529ms/step - loss: 0.2264 - accuracy: 0.9226 - val_loss: 0.5375 - val_accuracy: 0.8435\n",
            "Epoch 9/20\n",
            "702/702 [==============================] - 376s 533ms/step - loss: 0.2150 - accuracy: 0.9252 - val_loss: 0.5597 - val_accuracy: 0.8451\n",
            "Epoch 10/20\n",
            "702/702 [==============================] - 380s 538ms/step - loss: 0.2065 - accuracy: 0.9298 - val_loss: 0.5708 - val_accuracy: 0.8387\n",
            "Epoch 11/20\n",
            "702/702 [==============================] - 389s 550ms/step - loss: 0.1960 - accuracy: 0.9344 - val_loss: 0.5845 - val_accuracy: 0.8363\n",
            "Epoch 12/20\n",
            "702/702 [==============================] - 379s 536ms/step - loss: 0.1903 - accuracy: 0.9354 - val_loss: 0.6020 - val_accuracy: 0.8372\n",
            "Epoch 13/20\n",
            "702/702 [==============================] - 378s 535ms/step - loss: 0.1833 - accuracy: 0.9369 - val_loss: 0.6185 - val_accuracy: 0.8353\n",
            "Epoch 14/20\n",
            "702/702 [==============================] - 389s 551ms/step - loss: 0.1774 - accuracy: 0.9392 - val_loss: 0.6353 - val_accuracy: 0.8360\n",
            "Epoch 15/20\n",
            "702/702 [==============================] - 387s 548ms/step - loss: 0.1719 - accuracy: 0.9408 - val_loss: 0.6552 - val_accuracy: 0.8344\n",
            "Epoch 16/20\n",
            "702/702 [==============================] - 391s 554ms/step - loss: 0.1658 - accuracy: 0.9432 - val_loss: 0.6676 - val_accuracy: 0.8315\n",
            "Epoch 17/20\n",
            "702/702 [==============================] - 383s 543ms/step - loss: 0.1618 - accuracy: 0.9445 - val_loss: 0.6844 - val_accuracy: 0.8333\n",
            "Epoch 18/20\n",
            "702/702 [==============================] - 381s 539ms/step - loss: 0.1593 - accuracy: 0.9457 - val_loss: 0.6961 - val_accuracy: 0.8328\n",
            "Epoch 19/20\n",
            "702/702 [==============================] - 380s 538ms/step - loss: 0.1553 - accuracy: 0.9474 - val_loss: 0.7131 - val_accuracy: 0.8288\n",
            "Epoch 20/20\n",
            "702/702 [==============================] - 383s 542ms/step - loss: 0.1481 - accuracy: 0.9490 - val_loss: 0.7322 - val_accuracy: 0.8285\n"
          ]
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20\n",
        ")\n",
        "# Also add epochs that model has been trained on\n",
        "import sys\n",
        "original_stdout = sys.stdout # Save a reference to the original standard output\n",
        "with open(f'{model_dir}/{model_name}.txt', 'w') as f:\n",
        "    sys.stdout = f # Change the standard output to the file we created.\n",
        "    model.summary()\n",
        "    sys.stdout = original_stdout # Reset the standard output to its original value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRylmlDOu2M6"
      },
      "source": [
        "### Convert to tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BKRuf1nXuyxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a98dee-953d-444a-fed3-7099795e715e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ],
      "source": [
        "def representative_data_gen():\n",
        "  a = []\n",
        "  limit = 3\n",
        "  counter = 0\n",
        "  for batch in train_ds:\n",
        "    counter += 1\n",
        "    if counter == 3:\n",
        "      break\n",
        "    input_tensor = batch[0]\n",
        "    for i in range(input_tensor.shape[0]):\n",
        "      a.append(input_tensor[i])\n",
        "  a = np.asarray(a)\n",
        "  ds = tf.data.Dataset.from_tensor_slices(a).batch(1)\n",
        "  for i in ds.take(100):\n",
        "    yield [i]\n",
        "\n",
        "def write_to_file(text):\n",
        "  with open(f'{model_dir}/{model_name}.txt', 'a') as f:\n",
        "    f.write(text + '\\n')\n",
        "\n",
        "def no_quantization(converter):\n",
        "  write_to_file('quantization: none')\n",
        "  return converter.convert()\n",
        "\n",
        "# Smaller model with quantized weights, but other variable data still in float format\n",
        "# NOTE: Hybrid models are not supported in TFlite Micro\n",
        "def dynamic_quantization(converter):\n",
        "  write_to_file('quantization: dynamic')\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  return converter.convert()\n",
        "\n",
        "# All weights and variable data quantized\n",
        "# Not compatible with devices that perform only integer-based operations \n",
        "def float_quantization(converter):\n",
        "  write_to_file('quantization: float')\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.representative_dataset = representative_data_gen\n",
        "  return converter.convert()\n",
        "\n",
        "# All weights and variable data quantized\n",
        "# Internal quantization is the same as with float, but inuput and output tensors format is changed to uint8\n",
        "def int_quantization(converter):\n",
        "  write_to_file('quantization: int')\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.representative_dataset = representative_data_gen\n",
        "  # Ensure that if any ops can't be quantized, the converter throws an error\n",
        "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "  # Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "  converter.inference_input_type = tf.uint8\n",
        "  converter.inference_output_type = tf.uint8\n",
        "  return converter.convert()\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "if quantization == 'none':\n",
        "  tflite_model = no_quantization(converter)\n",
        "elif quantization == 'dynamic':\n",
        "  tflite_model = dynamic_quantization(converter)\n",
        "elif quantization == 'float':\n",
        "  tflite_model = float_quantization(converter)\n",
        "elif quantization == 'int':\n",
        "  tflite_model = int_quantization(converter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HheX71zbTzrE"
      },
      "source": [
        "### Check model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7OZII2_T2aU",
        "outputId": "6c3171f7-8680-4a13-a76a-19ebf6abc259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176/176 [==============================] - 79s 440ms/step - loss: 0.7322 - accuracy: 0.8285\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(val_ds)\n",
        "write_to_file(f'loss={loss}, accuracy={acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh8LYOisu7uk"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RGUC7Pn6u80l"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "model_save_name = f'{model_name}.tflite'\n",
        "\n",
        "tflite_model_file = model_dir/model_save_name\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "with open(f'{model_dir}/labels.txt', 'a') as f:\n",
        "    for class_name in train_ds.class_names:\n",
        "      f.write(class_name + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaQkCQ26vgQr"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KBub73dHvhUI"
      },
      "outputs": [],
      "source": [
        "# # Initialize the interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "# Load image\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "  'https://www.autofromchina.com/u_file/2208/products/05/17e85350c7.jpg.240x240.jpg',\n",
        "   \"gfg.jpg\")\n",
        "image = Image.open('gfg.jpg').resize((96,96))\n",
        "image = np.asarray(image)\n",
        "image = image.reshape((96, 96, 3))\n",
        "\n",
        "test_image = np.expand_dims(image, axis=0).astype(input_details[\"dtype\"])\n",
        "interpreter.set_tensor(input_details['index'], test_image)\n",
        "interpreter.invoke()\n",
        "\n",
        "output = interpreter.get_tensor(output_details['index'])[0]\n",
        "print(output)\n",
        "print(output.argmax())\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.imshow(image)\n",
        "\n",
        "im = Image.fromarray(image)\n",
        "im.save('veh.jpg')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyPvZ9kfI8KoXfa2/wiU6NeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}